{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fee790bc89cb036",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Wykrywanie Zaburze≈Ñ Sygna≈Çu EKG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6fec6",
   "metadata": {},
   "source": [
    "![ECG_anomaly_detection_intro.png](https://live.staticflickr.com/65535/54253200317_77c251c48c_o.png)\n",
    "\n",
    "*Obraz wygenerowany przy u≈ºyciu modelu DALL-E.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc76c62a8c7e5e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Wstƒôp\n",
    "\n",
    "Rozw√≥j sztucznej inteligencji otwiera nowe mo≈ºliwo≈õci w diagnostyce medycznej, zw≈Çaszcza w analizie z≈Ço≈ºonych danych, takich jak [sygna≈Çy elektrokardiograficzne (EKG)](https://www.healio.com/cardiology/learn-the-heart/ecg-review/ecg-interpretation-tutorial). EKG to jedno z najczƒô≈õciej stosowanych narzƒôdzi diagnostycznych w medycynie, umo≈ºliwiajƒÖce ocenƒô i wykrywanie nieprawid≈Çowo≈õci pracy serca.\n",
    "\n",
    "Tradycyjnie sygna≈Ç EKG powstaje z dwunastu odprowadze≈Ñ, jednak w tym zadaniu skupimy siƒô na sygnale jednoodprowadzeniowym, czyli dysponujemy\n",
    "jednƒÖ zmiennƒÖ reprezentujƒÖcƒÖ napiƒôcie elektryczne generowane przez serce w czasie. Dane te sƒÖ rejestrowane w postaci krzywej zale≈ºnej od czasu, czyli mo≈ºna tutaj m√≥wiƒá o szeregu czasowym. Z sygna≈Çu EKG mo≈ºna wydzieliƒá charakterystyczne fragmenty, czyli **za≈Çamki** (ang. waves) P, Q, R, S, T oraz **odstƒôpy** pomiƒôdzy dwoma zdarzeniami w EKG (ang. intervals), spo≈õr√≥d kt√≥rych istotnƒÖ rolƒô odgrywa odstƒôp R-R (czas miƒôdzy wystƒÖpieniem dw√≥ch kolejnych za≈Çamk√≥w R). Opr√≥cz tego m√≥wimy o **odcinkach** (ang. segments), czyli d≈Çugo≈õci miƒôdzy dwoma okre≈õlonymi za≈Çamkami w EKG, pomiƒôdzy kt√≥rymi powinna wystƒôpowaƒá bazowa amplituda sygna≈Çu. Z kolei **zesp√≥≈Ç** (ang. complex) stanowi kilka zgrupowanych za≈Çamk√≥w. G≈Ç√≥wnie wyr√≥zniamy tutaj zesp√≥≈Ç QRS. Schematyczny rysunek EKG wraz z podpisanymi fragmentami, jest prezentowany poni≈ºej.\n",
    "\n",
    "![ECG.png](https://live.staticflickr.com/65535/54254099351_213d47784d_o.png)\n",
    "\n",
    "W EKG pochodzƒÖcym od zdrowej osoby mo≈ºna zauwa≈ºyƒá sekwencjƒô PQRST. Na poczƒÖtku wyr√≥≈ºniamy za≈Çamek P, kt√≥ra reprezentuje skurcz przedsionk√≥w i jest ma≈Çym pionowym wychyleniem przed zespo≈Çem QRS. Nastƒôpnie, zesp√≥≈Ç QRS wskazuje na skurcz kom√≥r i tworzony jest przez trzy wygiƒôcia: za≈Çamek Q, za≈Çamek R oraz za≈Çamek S. Dalej mo≈ºna zauwa≈ºyƒá odcinek ST, czyli p≈Çaski odcinek miƒôdzy zespo≈Çem QRS a za≈Çamkiem T, kt√≥ry odpowiada wczesnej fazie repolaryzacji kom√≥r. Finalnie, za≈Çamek T, kt√≥ry jest zaokrƒÖglonym, pionowym wychyleniem, dotyczy repolaryzacji kom√≥r i ich powrotu do wyj≈õciowego stanu.  Sekwencja PQRST przypomina sinusoidƒô, kt√≥rej maksimum jest osiƒÖgane dla za≈Çamka R. W przypadku zaburze≈Ñ pracy serca, EKG mo≈ºe wykazywaƒá r√≥≈ºne anomalie, takie jak dodatkowe minima lub maksima, czy te≈º znacznie zwiƒôkszone odchylenie standardowe podczas ca≈Çego pomiaru. Charakterystyka tych anomalii zale≈ºy od rodzaju i przyczyny zaburzenia.\n",
    "\n",
    "W poni≈ºszym zadaniu musisz zmierzyƒá siƒô z pr√≥bkami zawierajƒÖcymi pojedyncze sekwencje PQRST oraz ich okolice. Wiƒôkszo≈õƒá pr√≥bek bƒôdzie odpowiada≈Ça danym bez anomalii, kt√≥rych przyk≈Çad zaprezentowany jest na poni≈ºszym obrazku:\n",
    "\n",
    "![normal_example.png](https://live.staticflickr.com/65535/54253200322_f0173af129_o.png)\n",
    "\n",
    "Wystƒôpowaƒá bƒôdƒÖ te≈º pomiary odpowiadajƒÖce czterem rodzajom zaburze≈Ñ:  **AFib**, czyli migotaniu przedsionk√≥w, **PAC**, czyli przedwczesnemu pobudzeniu przedsionkowemu, **PVC**, czyli przedwczesnym skurczom komorowym oraz **STEMI**, czyli zawa≈Çowi miƒô≈õnia sercowego z uniesieniem odcinka ST.\n",
    "\n",
    "**UWAGA**: Poni≈ºsze dane sƒÖ danymi syntetycznymi i sƒÖ tylko pewnym przybli≈ºeniem rzeczywistych danych EKG!\n",
    "\n",
    "EKG jest typowym przyk≈Çadem szeregu czasowego, kt√≥ry mo≈ºna analizowaƒá za pomocƒÖ dedykowanych metod uczenia maszynowego, w tym sieci neuronowych, np. sieci rekurencyjnych. Jednak nie zawsze wykorzystanie sieci neuronowych jest konieczne, a nawet wskazane. W przypadku niekt√≥rych problem√≥w satysfakcjonujƒÖce wyniki mo≈ºna uzyskaƒá za pomocƒÖ prostszych metod, gdzie kluczowe jest odpowiednie przygotowanie danych. Ich umiejƒôtna analiza pozwala na selekcjƒô kliku metacech - cech zwiƒô≈∫le opisujƒÖcych pr√≥bki ze zbioru danych, np. ≈õredniej, minimum, maksimum, odchylenia standardowego, itp. MogƒÖ byƒá one wykorzystane do klasyfikacji zamiast oryginalnych cech. Dziƒôki temu korzystamy z niskowymiarowych danych wej≈õciowych, przyk≈Çadowo redukujemy 150-wymiarowy wektor zawierajƒÖcy informacje z oryginalnych krok√≥w czasowych do wektora 4-wymiarowego zawierajƒÖcego specjalnie przygotowane cechy.\n",
    "\n",
    "Przyk≈Çadem zastosowania niewielkiej liczby metacech sƒÖ modele uczenia maszynowego, kt√≥re majƒÖ dzia≈Çaƒá na urzƒÖdzeniach wbudowanych lub ma≈Çych urzƒÖdzeniach mobilnych, gdzie kluczowe sƒÖ takie ograniczenia, jak wym√≥g niskiego poboru energii, ma≈Ça ilo≈õƒá dostƒôpnej pamiƒôci operacyjnej czy ograniczona moc obliczeniowa. W takich przypadkach wymagane jest zastosowanie prostszych modeli, kt√≥re sƒÖ w stanie zapewniƒá odpowiedniƒÖ dok≈Çadno≈õƒá klasyfikacji przy jednoczesnym zachowaniu wymaganych ogranicze≈Ñ.\n",
    "\n",
    "## Zadanie\n",
    "\n",
    "Przygotuj rozwiƒÖzanie (wraz z wytrenowaniem modelu lasu losowego), kt√≥re spe≈Çni wymagania naszego urzƒÖdzenia wbudowanego. Przeanalizuj dane i przygotuj zestaw **4 metacech**, kt√≥re dadzƒÖ najlepszƒÖ zr√≥wnowa≈ºonƒÖ dok≈Çadno≈õƒá (ang. *balanced accuracy*) dla problemu klasyfikacji sygna≈Ç√≥w EKG. Zbi√≥r danych sk≈Çada siƒô ze zbioru treningowego oraz walidacyjnego (wraz z etykietami), na kt√≥rym mo≈ºesz weryfikowaƒá swoje podej≈õcie. Twoje rozwiƒÖzanie bƒôdzie sprawdzane na osobnym (tajnym) zbiorze testowym, w kt√≥rym liczba obserwacji bƒôdzie siƒô r√≥≈ºniƒá od liczby obserwacji w zbiorach treningowym i walidacyjnym. Ka≈ºda pr√≥bka jest opisana 150 warto≈õciami odpowiadajƒÖcymi kolejnym krokom czasowym oraz jest przypisana do jednej z piƒôciu nastƒôpujƒÖcych klas:\n",
    "\n",
    "| ID klasy  | Nazwa klasy   | Opis  | Pr√≥bki w zbiorze treningowym | Pr√≥bki w zbiorze walidacyjnym |\n",
    "| ------    | ------        | ----  | ---------------------------- | ----------------------------- |\n",
    "| 0         | normal        | brak anomalii | 1400 | 819 |\n",
    "| 1         | afib          | Atrial Fibrillation (migotanie przedsionk√≥w)| 150 | 142 |\n",
    "| 2         | pac           | Premature Atrial Contractions (przedwczesne pobudzenie przedsionkowe)| 150 | 191 |\n",
    "| 3         | pvc           | Premature Ventricular Complex (przedwczesne skurcze komorowe)| 150 | 197 |\n",
    "| 4         | st_elevation  | ST-elevation myocardial infarction (zawa≈Ç miƒô≈õnia sercowego z uniesieniem odcinka ST) | 150 | 151 |\n",
    "\n",
    "Nowe cechy powinny zawieraƒá kluczowe informacje diagnostyczne r√≥znicujƒÖce powy≈ºsze klasy, kt√≥re pozwolƒÖ na skutecznƒÖ klasyfikacjƒô wymienionych anomalii.\n",
    "\n",
    "Klasyfikatorem dla tego zadania jest [las losowy](https://pl.wikipedia.org/wiki/Las_losowy) z liczbƒÖ drzew decyzyjnych nie wiƒôkszƒÖ ni≈º 10 oraz maksymalnƒÖ g≈Çƒôboko≈õciƒÖ 10. **RozwiƒÖzania niespe≈ÇniajƒÖce tych warunk√≥w bƒôdƒÖ dyskwalifikowane!** W przypadku innych parametr√≥w lasu nie ma ogranicze≈Ñ. Dozwolony jest tak≈ºe preprocessing, czyli wstƒôpne przetwarzanie danych wej≈õciowych (np. zastosowanie normalizacji danych).\n",
    "\n",
    "### Kryterium Oceny\n",
    "\n",
    "Twoje rozwiƒÖzanie oceniane bƒôdzie na tajnym zbiorze testowym na podstawie [zr√≥wnowa≈ºonej dok≈Çadno≈õci klasyfikacji (balanced accuracy)](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.balanced_accuracy_score.html):\n",
    "\n",
    "$$\\text{score}(balanced\\_accuracy) = \n",
    "\\begin{cases} \n",
    "    0 &\\quad \\text{je≈ºeli }  balanced\\_accuracy \\leq 75 \\% \\\\\n",
    "    100 &\\quad \\text{je≈ºeli }  balanced\\_accuracy \\geq 98 \\% \\\\\n",
    "    \\dfrac{balanced\\_accuracy - 75 \\%}{98 \\% - 75 \\%} &\\quad \\text{w pozosta≈Çych przypadkach}\n",
    "\\end{cases}$$\n",
    "\n",
    "Oznacza to, ≈ºe wszystkie rozwiƒÖzania, kt√≥re na zbiorze testowym uzyskajƒÖ do $75\\%$ zr√≥wnowa≈ºonej dok≈Çadno≈õci klasyfikacji, otrzymajƒÖ $0$ punkt√≥w, za≈õ co najmniej $98\\%$ zr√≥wnowa≈ºonej dok≈Çadno≈õci klasyfikacji, uzyskajƒÖ maksymalnƒÖ liczbƒô punkt√≥w za zadanie. Wszystkie za≈õ¬†warto≈õci z przedzia≈Çu $75-98\\%$ zostanƒÖ zamienione na liczbƒô punkt√≥w (miƒôdzy $0$ a $100$) zgodnie z powy≈ºszym wzorem.\n",
    "\n",
    "*Wskaz√≥wka*: Twoim wyznacznikiem jako≈õci proponowanego rozwiƒÖzania powinien byƒá wynik na zbiorze walidacyjnym.\n",
    "\n",
    "W zagadnieniach dot. wykrywania chor√≥b do≈õƒá czƒôsto mamy do czynienia z niezr√≥wnowa≈ºonym (niezbalansowanym) zbiorem danych. Chodzi o to, ≈ºe zazwyczaj w≈õr√≥d danych dominujƒÖ przyk≈Çady *normalne*, odpowiadajƒÖce osobom zdrowym, a pr√≥bki reprezentujƒÖce osoby chore zwykle nale≈ºƒÖ do mniejszo≈õci. Wyobra≈∫my sobie sytuacjƒô, w kt√≥rej na 100 pr√≥bek jedynie 10 dotyczy os√≥b chorych, a pozosta≈Çe 90 zdrowych. W√≥wczas model, kt√≥ry ka≈ºdej pr√≥bce przyporzƒÖdkowywa≈Çby klasƒô *zdrowy*, osiƒÖgnƒÖ≈Çby 90% dok≈Çadno≈õci klasyfikacji, lecz jedynie 50% zr√≥wnowa≈ºonej dok≈Çadno≈õci klasyfikacji! Oczywi≈õcie taki model by≈Çby bezu≈ºyteczny. W takich przypadkach potrzebujemy miary, kt√≥ra lepiej odpowiada potrzebom wynikajƒÖcym z postawionego problemu i informuje o skuteczno≈õci modelu w spos√≥b u≈ºyteczny z punktu widzenia jego p√≥≈∫niejszego u≈ºytkownika.\n",
    "\n",
    "W tym zadaniu musisz siƒô wiƒôc skupiƒá siƒô na tym, by ka≈ºda z klas by≈Ça przyporzƒÖdkowywana prawid≈Çowo.\n",
    "\n",
    "## Ograniczenia\n",
    "- Twoje rozwiazanie bƒôdzie testowane na Platformie Konkursowej bez dostƒôpu do internetu oraz w ≈õrodowisku bez GPU.\n",
    "- Ewaluacja Twojego finalnego rozwiƒÖzania na Platformie Konkursowej nie mo≈ºe trwaƒá d≈Çu≈ºej ni≈º 1 minutƒô bez GPU.\n",
    "- Podczas przygotowania danych nale≈ºy pamiƒôtaƒá o tym, ≈ºe:\n",
    "    - zakazane jest korzystanie z innych ni≈º lasy losowe metod uczenia maszynowego, zar√≥wno nadzorowanego jak i nienadzorowanego (np. autokodery, wielowarstwowe perceptrony i inne sieci neuronowe, maszyny wektor√≥w no≈õnych (SVM), i inne), dozwolone sƒÖ jednak metody redukcji wymiarowo≈õci, w stylu analizy sk≈Çadowych g≈Ç√≥wnych (PCA);\n",
    "    - przy konstrukcji metacech mo≈ºna korzystaƒá wy≈ÇƒÖcznie z funkcji dostƒôpnych standardowo w Pythonie (`v3.11`), a tak≈ºe Numpy (`v2.0.2`) oraz Scipy (`v1.14.1`);\n",
    "    - mo≈ºna wyznaczyƒá maksymalnie 4 metacechy,\n",
    "- Do klasyfikacji mo≈ºna wykorzystaƒá wy≈ÇƒÖcznie [las losowy (RandomForestClassifier) z biblioteki scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) (`v1.5.2`):\n",
    "    - z≈Ço≈ºony z maksymalnie 10 drzew decyzyjnych (`n_estimators` $\\leq 10$);\n",
    "    - ka≈ºde drzewo ma mieƒá maksymalnƒÖ g≈Çƒôboko≈õƒá r√≥wnƒÖ 10 (`max_depth` $\\leq 10$);\n",
    "    - pozosta≈Çe hiperparametry mo≈ºna modyfikowaƒá bez ogranicze≈Ñ;\n",
    "\n",
    "## Pliki Zg≈Çoszeniowe\n",
    "Ten notebook uzupe≈Çniony o Twoje rozwiƒÖzanie (patrz klasa `YourSolution`), w kt√≥rym przygotujesz zestaw 4 metacech opisujƒÖcych zbi√≥r danych oraz zestaw hiperparametr√≥w lasu losowego.\n",
    "\n",
    "## Ewaluacja\n",
    "Pamiƒôtaj, ≈ºe podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`.\n",
    "\n",
    "Za to zadanie mo≈ºesz zdobyƒá pomiƒôdzy 0 a 100 punkt√≥w. Liczba punkt√≥w, kt√≥rƒÖ zdobƒôdziesz, bƒôdzie wyliczona na (tajnym) zbiorze testowym na Platformie Konkursowej na podstawie wy≈ºej wspomnianego wzoru, zaokrƒÖglona do liczby ca≈Çkowitej. Je≈õli Twoje rozwiƒÖzanie nie bƒôdzie spe≈Çnia≈Ço powy≈ºszych kryteri√≥w lub nie bƒôdzie wykonywaƒá siƒô prawid≈Çowo, otrzymasz za zadanie 0 punkt√≥w.\n",
    "\n",
    "---\n",
    "\n",
    "## Informacje Uzupe≈ÇniajƒÖce\n",
    "\n",
    "### Zr√≥wnowa≈ºona Dok≈Çadno≈õƒá Klasyfikacji\n",
    "\n",
    "Niech $C$ bƒôdzie liczbƒÖ klas, a $N_j$ odpowiada ilo≈õci pr√≥bek nale≈ºƒÖcych do $j$-tej klasy, gdzie $j \\in \\lbrace 1, ..., C \\rbrace$. Ponadto, niech $\\hat{y}_{i,j}$ bƒôdzie przewidywanƒÖ przez model klasƒÖ dla $j$-tej pr√≥bki nale≈ºƒÖcej w rzeczywisto≈õci do $i$-tej klasy. W√≥wczas zr√≥wnowa≈ºonƒÖ (zbalansowanƒÖ) dok≈Çadno≈õƒá klasyfikacji mo≈ºemy wyliczyƒá nastƒôpujƒÖco:\n",
    "\n",
    "$$\n",
    "balanced\\_accuracy = \\dfrac{1}{C} \\sum\\limits_{i=1}^{C} \\sum\\limits_{j=1}^{|N_c|} \\dfrac{1}{|N_c|} \\cdot \\mathbf{1} \\left( \\hat{y}_{i, j} = i \\right),\n",
    "$$\n",
    "\n",
    "gdzie $\\mathbf{1} \\left( \\hat{y}_{i, j} = i \\right)$ jest funkcjƒÖ indykatorowƒÖ, kt√≥ra przyjmuje warto≈õƒá 1, je≈õli $\\hat{y}_{i, j} = i$, czyli w sytuacji, w kt√≥rej klasa przewidywana dla $j$-tej pr√≥bki jest taka sama jak rzeczywista klasa tej pr√≥bki oraz 0 w przeciwnym przypadku. Suma zewnƒôtrzna przebiega po kolejnych klasach, a wewnƒôtrzna po kolejnych pr√≥bkach nale≈ºƒÖcych do danej klasy.\n",
    "\n",
    "**Przyk≈Çad**: Niech\n",
    "$$\\mathbf{y} = [0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3]$$\n",
    "\n",
    "bƒôdzie wektorem reprezentujƒÖcym rzeczywiste klasy dla kolejnych pr√≥bek, a\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = [0, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3]$$\n",
    "\n",
    "wektorem reprezentujƒÖcym predykcje modelu dla tych≈ºe pr√≥bek. Mamy wiƒôc do czynienia z czterema klasami, gdzie model mia≈Ç problem z klasƒÖ o numerze $1$. Wszystkie pozosta≈Çe przyk≈Çady zosta≈Çy przypisane bezb≈Çƒôdnie. ≈ÅƒÖcznie 10 na 12 pr√≥bek zosta≈Ço sklasyfikowanych prawid≈Çowo, co oznacza, ≈ºe gdyby≈õmy mieli mierzyƒá \"zwyk≈ÇƒÖ\" dok≈Çadno≈õƒá klasyfikacji, otrzymaliby≈õmy ok. $83.3\\%$ . Jednak gdy przyjrzymy siƒô zbalansowanej dok≈Çadno≈õci klasyfikacji, otrzymamy wynik $75\\%$.\n",
    "\n",
    "Za≈Ç√≥≈ºmy teraz, ≈ºe\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = [0, 0, 0, 1, 0, 2, 2, 2, 2, 3, 3, 3]$$\n",
    "\n",
    "czyli model klasyfikuje poprawnie $50\\%$ pr√≥bek z klasy 1 oraz $100\\%$ pr√≥bek z pozosta≈Çych klas. \"Zwyk≈Ça\" dok≈Çadno≈õƒá klasyfikacji wynosi tutaj niespe≈Çna $92\\%$, podczas gdy zbalansowana dok≈Çadno≈õƒá klasyfikacji wynosi $87.5\\%$.\n",
    "\n",
    "### Anomalie WystƒôpujƒÖce w Rozwa≈ºanym Zbiorze Danych\n",
    "\n",
    "#### AFib\n",
    "**Atrial Fibrillation (AFib)**, czyli migotanie przedsionk√≥w, wystƒôpuje wtedy, gdy potencja≈Çy czynno≈õciowe wyzwalajƒÖ siƒô bardzo szybko i chaotycznie, w zwiƒÖzku z tym rytm serca jest nieregularny. W tym zaburzeniu za≈Çamki P mogƒÖ nie byƒá widoczne w EKG, a zesp√≥≈Ç QRS staje siƒô nieregularny.\n",
    "\n",
    "![AFIB_example.png](https://live.staticflickr.com/65535/54253219357_78c51f06ff_o.png)\n",
    "\n",
    "#### PAC\n",
    "**Premature Atrial Contractions (PACs)**, czyli przedwczesne pobudzenie przedsionkowe, zwiƒÖzane jest z nieprawid≈Çowym za≈Çamkiem P, po kt√≥rym nastƒôpuje prawid≈Çowy zesp√≥≈Ç QRS. *Uwaga!* W pr√≥bkach z zadania wystƒôpujƒÖ przyk≈Çady, w kt√≥rych w ramach jednej pr√≥bki ze zbioru danych widoczny jest jedynie przedwczesny za≈Çamek P.\n",
    "\n",
    "![PAC_example.png](https://live.staticflickr.com/65535/54254324138_df77fdd62a_o.png)\n",
    "\n",
    "#### PVC\n",
    "**Premature Ventricular Contractions (PVCs)**, czyli przedwczesne skurcze komorowe, sƒÖ dodatkowymi uderzeniami serca, kt√≥re rozpoczynajƒÖ siƒô w jednej z dw√≥ch kom√≥r serca i zak≈Ç√≥cajƒÖ jego regularny rytm. SƒÖ jednym z powszechnych rodzaj√≥w arytmii. Skurcze te zachodzƒÖ wcze≈õniej ni≈º by≈Çoby to oczekiwane na podstawie poprzednich odstƒôp√≥w R-R.\n",
    "\n",
    "![PVC_example.png](https://live.staticflickr.com/65535/54254518540_12ba23c53f_o.png)\n",
    "\n",
    "#### STEMI\n",
    "**ST-elevation myocardial infarction (STEMI)**, czyli zawa≈Ç miƒô≈õnia sercowego z uniesieniem odcinka ST, powoduje zablokowanie przep≈Çywu krwi do miƒô≈õnia sercowego i jego obumarcie. Segment ST wystƒôpuje tu≈º po zespole QRS. W normalnej sytuacji nie ma tam ≈ºadnej aktywno≈õci elektrycznej, przez co jest on p≈Çaski. Je≈õli za≈õ wystƒôpuje uniesienie odcinka ST, oznacza to blokadƒô jednej z g≈Ç√≥wnych tƒôtnic doprowadzajƒÖcych krew do serca.\n",
    "\n",
    "![STEMI_example.png](https://live.staticflickr.com/65535/54254099356_422c23144e_o.png)\n",
    "\n",
    "---\n",
    "\n",
    "**≈πr√≥d≈Ça opisu medycznego dot. EKG**: [1](https://www.healio.com/cardiology/learn-the-heart/ecg-review/ecg-topic-reviews-and-criteria/premature-ventricular-contractions-review), [2](https://www.mayoclinic.org/diseases-conditions/premature-ventricular-contractions/symptoms-causes/syc-20376757), [3](https://litfl.com/premature-atrial-complex-pac/), [4](https://www.healio.com/cardiology/learn-the-heart/ecg-review/ecg-topic-reviews-and-criteria/premature-atrial-contractions-review), [5](https://www.mayoclinic.org/diseases-conditions/atrial-fibrillation/symptoms-causes/syc-20350624), [6](https://www.healio.com/cardiology/learn-the-heart/ecg-review/ecg-topic-reviews-and-criteria/atrial-fibrillation-review), [7](https://my.clevelandclinic.org/health/diseases/22068-stemi-heart-attack), obraz za≈Çamk√≥w PQRST na podstawie [8](https://www.sciencedirect.com/science/article/pii/S0213911121002466)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221e864-03ae-4230-8a37-bfec058ff3ac",
   "metadata": {},
   "source": [
    "# Kod Startowy\n",
    "\n",
    "W tej sekcji inicjalizujemy ≈õrodowisko poprzez zaimportowanie potrzebnych bibliotek i funkcji. Przygotowany kod u≈Çatwi Tobie efektywne operowanie na danych i budowanie w≈Ça≈õciwego rozwiƒÖzania."
   ]
  },
  {
   "cell_type": "code",
   "id": "a4053c4df3efc18b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOM√ìRKI PODCZAS WYSY≈ÅANIA ##########################\n",
    "\n",
    "# W czasie sprawdzania Twojego rozwiƒÖzania, warto≈õƒá flagi FINAL_EVALUATION_MODE zostanie zmieniona na True\n",
    "FINAL_EVALUATION_MODE = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8511460ed63f204e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOM√ìRKI PODCZAS WYSY≈ÅANIA ##########################\n",
    "import cloudpickle\n",
    "\n",
    "import os\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82c4f388e11fc3ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOM√ìRKI PODCZAS WYSY≈ÅANIA ##########################\n",
    "\n",
    "# Ustawienie ziarna generatora liczb pseudolosowych w celu zapewnienia deterministyczno≈õci wynik√≥w.\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6ae949692319123",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## ≈Åadowanie Danych\n",
    "Za pomocƒÖ poni≈ºszego kodu wczytujemy dane."
   ]
  },
  {
   "cell_type": "code",
   "id": "aceaa260cc57432c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOM√ìRKI PODCZAS WYSY≈ÅANIA ##########################\n",
    "\n",
    "train_val_filename = \"train_validation_sets.npz\"\n",
    "if not os.path.exists(train_val_filename):\n",
    "    import gdown\n",
    "    url = \"https://drive.google.com/file/d/1pCqgbsKBQP1UnH2kMmBKRS1AuvmSl9jx/view?usp=sharing\"\n",
    "    gdown.download(url, train_val_filename, quiet=True, fuzzy=True)\n",
    "\n",
    "train_valid_bundle = np.load(\"train_validation_sets.npz\", allow_pickle=True)\n",
    "x_train = train_valid_bundle[\"X_train\"]\n",
    "y_train = train_valid_bundle[\"y_train\"]\n",
    "y_train_str = train_valid_bundle[\"anomaly_train\"]\n",
    "\n",
    "x_valid = train_valid_bundle[\"X_validation\"]\n",
    "y_valid = train_valid_bundle[\"y_validation\"]\n",
    "y_valid_str = train_valid_bundle[\"anomaly_validation\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39ff3878366476e8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Publiczny Interfejs RozwiƒÖzania\n",
    "\n",
    "Tylko tego wymagamy od Twojej klasy. W Twoim rozwiƒÖzaniu mo≈ºesz modyfikowaƒá swojƒÖ klasƒô do woli dodajƒÖc atrybuty oraz nowe metody obliczajƒÖce metacechy - cokolwiek co bƒôdzie Ci potrzebne do rozwiƒÖzania zadania."
   ]
  },
  {
   "cell_type": "code",
   "id": "55f895fcd12becb3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOM√ìRKI PODCZAS WYSY≈ÅANIA ##########################\n",
    "\n",
    "class ISolution(ABC):\n",
    "    random_forest: RandomForestClassifier | None = None\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_training(cls) -> \"ISolution\":\n",
    "        \"\"\"Metoda s≈Çu≈ºƒÖca do stworzenia rozwiƒÖzania z wytrenowanym lasem losowym.\"\"\"\n",
    "        solution = cls()\n",
    "\n",
    "        hyperparameters = cls.get_rf_hyperparameters()\n",
    "        hyperparameters = cls.validate_hyperparameters(hyperparameters)\n",
    "        solution.random_forest = RandomForestClassifier(**hyperparameters)\n",
    "\n",
    "        meta_features = solution.compute_meta_features(x_train)\n",
    "        solution.random_forest.fit(meta_features, y_train)\n",
    "        return solution\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_hyperparameters(hyperparameters: dict[str, int | float | str]) -> dict[str, int | float | str]:\n",
    "        \"\"\"\n",
    "        Funkcja ta sprawdza, czy hiperparametry lasu losowego sƒÖ zgodne z wymaganiami zadania. Je≈õli nie, to poprawia je na\n",
    "        domy≈õlne warto≈õci.\n",
    "        \"\"\"\n",
    "        hyperparameters[\"n_estimators\"] = min(hyperparameters.get(\"n_estimators\", 10), 10)\n",
    "        hyperparameters[\"max_depth\"] = min(hyperparameters.get(\"max_depth\", 10), 10)\n",
    "        hyperparameters[\"random_state\"] = 42\n",
    "        return hyperparameters\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_meta_features(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Funkcja ta powinna dla ka≈ºdego przyk≈Çadu ze zbioru $x$ opisanego 150 cechami zwr√≥ciƒá wektor 4 cech, kt√≥ry bƒôdzie\n",
    "        reprezentowa≈Ç ten przyk≈Çad. Funkcja ta powinna przekszta≈Çcaƒá wej≈õciowƒÖ tablicƒô o rozmiarze (n, 150) na tablicƒô o\n",
    "        rozmiarze (n, 4).\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def get_rf_hyperparameters() -> dict[str, int | float | str]:\n",
    "        \"\"\"\n",
    "        Funkcja ta powinna zwracaƒá s≈Çownik z hiperparametrami lasu losowego. Pamiƒôtaj o ograniczeniach na liczbƒô drzew i ich\n",
    "        g≈Çƒôboko≈õƒá!\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "76fa659473771fa",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Kod z Kryterium OceniajƒÖcym\n",
    "Kod, zbli≈ºony do poni≈ºszego, bƒôdzie u≈ºywany do oceny rozwiƒÖzania na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "id": "621097e2336ffec3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOM√ìRKI PODCZAS WYSY≈ÅANIA ##########################\n",
    "\n",
    "def balanced_accuracy_to_score(balanced_accuracy: float) -> float:\n",
    "    return min(max((balanced_accuracy - 75.) * (100. / (98. - 75.)), 0.), 100.)\n",
    "\n",
    "\n",
    "def score_solution(solution: ISolution) -> float:\n",
    "    x, y = x_valid, y_valid\n",
    "    meta_features = solution.compute_meta_features(x)\n",
    "    y_hat = solution.random_forest.predict(meta_features)\n",
    "    balanced_accuracy = 100. * balanced_accuracy_score(y, y_hat)\n",
    "\n",
    "    assert meta_features.shape[-1] <= 4\n",
    "    assert solution.random_forest.n_estimators <= 10\n",
    "    assert solution.random_forest.max_depth <= 10\n",
    "\n",
    "    if not FINAL_EVALUATION_MODE:\n",
    "        print(\"Ocena dzia≈Çania modelu: \\n\")\n",
    "        print(f\"Zbalansowana dok≈Çadno≈õƒá klasyfikacji: {balanced_accuracy: .4f}\")\n",
    "    return int(round(balanced_accuracy_to_score(balanced_accuracy)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# from scipy.signal import find_peaks, peak_widths\n",
    "# from scipy.stats import variation, entropy, skew\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# \n",
    "# # Temporary class for dev ‚Äî skip assert limits\n",
    "# class DebugSolution:\n",
    "#     def compute_meta_features(self, x):\n",
    "#         n = x.shape[0]\n",
    "#         features = np.zeros((n, 10))  # We compute 10 features here\n",
    "#         \n",
    "#         for i in range(n):\n",
    "#             sig = x[i]\n",
    "#             norm_sig = (sig - sig.min()) / (sig.max() - sig.min() + 1e-8)\n",
    "#             peaks, _ = find_peaks(sig, height=0.4, distance=20)\n",
    "#             rr = np.diff(peaks)\n",
    "# \n",
    "#             # Original + new features\n",
    "#             features[i, 0] = variation(rr) if len(rr) > 1 else 0  # RR CV\n",
    "#             features[i, 1] = entropy(norm_sig + 1e-8)              # Entropy\n",
    "#             features[i, 2] = np.abs(skew(sig))                    # Skewness\n",
    "#             features[i, 3] = np.std(rr) if len(rr) > 1 else 0     # RR Std\n",
    "#             features[i, 4] = len(peaks)                           # Peak count\n",
    "#             features[i, 5] = np.max(sig) - np.min(sig)            # Peak-to-peak amp\n",
    "#             features[i, 6] = np.median(np.diff(sig))              # Median diff\n",
    "#             features[i, 7] = np.percentile(sig, 90) - np.percentile(sig, 10)  # Spread\n",
    "#             features[i, 8] = np.mean(np.abs(np.diff(sig)))        # Mean abs diff\n",
    "#             features[i, 9] = np.max(sig[:50]) - np.median(sig[:50])  # P-wave bump\n",
    "# \n",
    "#         return features\n",
    "# \n",
    "# # === Load your dev data ===\n",
    "# x_train, y_train = x_train, y_train\n",
    "# x_valid, y_valid = x_valid, y_valid\n",
    "# \n",
    "# # === Compute features ===\n",
    "# model = DebugSolution()\n",
    "# X_train_full = model.compute_meta_features(x_train)\n",
    "# X_valid_full = model.compute_meta_features(x_valid)\n",
    "# \n",
    "# # === Feature selection ===\n",
    "# selector = SelectKBest(f_classif, k=4)\n",
    "# X_train_sel = selector.fit_transform(X_train_full, y_train)\n",
    "# X_valid_sel = selector.transform(X_valid_full)\n",
    "# \n",
    "# top_features = selector.get_support(indices=True)\n",
    "# print(\"Selected top feature indices:\", top_features)\n",
    "# \n",
    "# # === Train classifier ===\n",
    "# clf = RandomForestClassifier(n_estimators=10, max_depth=10, random_state=42)\n",
    "# clf.fit(X_train_sel, y_train)\n",
    "# y_pred = clf.predict(X_valid_sel)\n",
    "# \n",
    "# # === Metrics ===\n",
    "# print(\"\\n=== Classification Report ===\")\n",
    "# print(classification_report(y_valid, y_pred, target_names=[\"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"]))\n",
    "# \n",
    "# # === Confusion Matrix ===\n",
    "# cm = confusion_matrix(y_valid, y_pred, normalize='true')\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# sns.heatmap(cm, annot=True, cmap=\"viridis\", xticklabels=[\"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"],\n",
    "#             yticklabels=[\"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"])\n",
    "# plt.title(\"Normalized Confusion Matrix\")\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"True\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# \n",
    "# # === Feature Importances ===\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.bar(range(4), clf.feature_importances_)\n",
    "# plt.xticks(range(4), [f'F{i}' for i in top_features])\n",
    "# plt.title(\"Top 4 Feature Importances\")\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ],
   "id": "3388c31e1d294f11",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a866b78776f10fe9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Przyk≈Çadowe RozwiƒÖzanie\n",
    "\n",
    "Poni≈ºej przedstawiamy uproszczone rozwiƒÖzanie, kt√≥re s≈Çu≈ºy jako przyk≈Çad demonstrujƒÖcy podstawowƒÖ funkcjonalno≈õƒá notatnika. Mo≈ºe ono pos≈Çu≈ºyƒá jako punkt wyj≈õcia do opracowania Twojego rozwiƒÖzania."
   ]
  },
  {
   "cell_type": "code",
   "id": "f561bb976552807f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "from scipy.stats import variation, skew, entropy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ExemplarySolution(ISolution):\n",
    "    def __init__(self):\n",
    "        self.selected_indices = None\n",
    "        self.random_forest = None\n",
    "        self.feature_names = [\n",
    "            \"RR_var\", \"Entropy\", \"Pwave_ratio\", \"P_early_dist\",\n",
    "            \"QRS_width\", \"QRS_sharp\", \"ST_elev\", \"Skew_abs\", \"Peak_count\",\n",
    "            \"RMS\", \"Kurtosis\", \"ST_slope\"\n",
    "        ]\n",
    "\n",
    "    def compute_all_features(self, x: np.ndarray) -> np.ndarray:\n",
    "        n_samples = x.shape[0]\n",
    "        features = np.zeros((n_samples, 12))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            signal = x[i]\n",
    "            norm_signal = (signal - np.min(signal)) / (np.max(signal) - np.min(signal) + 1e-8)\n",
    "\n",
    "            # R peaks\n",
    "            peaks, _ = find_peaks(signal, height=0.4, distance=20, prominence=0.2)\n",
    "            rr_intervals = np.diff(peaks) if len(peaks) >= 2 else np.array([1])\n",
    "\n",
    "            # === AFib ===\n",
    "            features[i, 0] = np.log1p(variation(rr_intervals)) if len(rr_intervals) > 1 else 0\n",
    "            features[i, 1] = entropy(norm_signal + 1e-8)\n",
    "\n",
    "            # === PAC ===\n",
    "            p_region = signal[:80]\n",
    "            p_peaks, p_props = find_peaks(p_region, height=0.2, prominence=0.1, width=5)\n",
    "            if len(p_peaks) > 0:\n",
    "                widths = peak_widths(p_region, p_peaks, rel_height=0.5)[0]\n",
    "                features[i, 2] = np.mean(p_props['prominences']) / (np.mean(widths) + 1e-6)\n",
    "                features[i, 3] = np.min(np.diff(p_peaks)) if len(p_peaks) > 1 else 0\n",
    "            else:\n",
    "                features[i, 2:4] = 0\n",
    "\n",
    "            # === PVC ===\n",
    "            qrs_peaks, _ = find_peaks(signal, height=0.5, distance=25, prominence=0.3)\n",
    "            if len(qrs_peaks) > 0:\n",
    "                main_peak = qrs_peaks[np.argmax(signal[qrs_peaks])]\n",
    "                width = peak_widths(signal, [main_peak], rel_height=0.5)[0][0]\n",
    "                features[i, 4] = width\n",
    "                features[i, 5] = signal[main_peak] / (width + 1e-6)\n",
    "            else:\n",
    "                features[i, 4:6] = 0\n",
    "                main_peak = np.argmax(signal[50:100]) + 50  # fallback\n",
    "\n",
    "            # === ST Elevation ===\n",
    "            qrs_end = main_peak + 15\n",
    "            st_region = signal[qrs_end:min(len(signal), qrs_end + 20)]\n",
    "            baseline = np.percentile(signal[:30], 25)\n",
    "            features[i, 6] = np.percentile(st_region, 90) - baseline\n",
    "\n",
    "            # === General ===\n",
    "            features[i, 7] = np.abs(skew(signal))\n",
    "            features[i, 8] = len(peaks)\n",
    "            features[i, 9] = np.sqrt(np.mean(signal**2))\n",
    "            features[i, 10] = kurtosis(signal)\n",
    "            st_len = len(st_region)\n",
    "            features[i, 11] = (st_region[-1] - st_region[0]) / (st_len if st_len else 1)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def compute_meta_features(self, x: np.ndarray) -> np.ndarray:\n",
    "        full = self.compute_all_features(x)\n",
    "        return full[:, self.selected_indices] if self.selected_indices is not None else full\n",
    "\n",
    "    @staticmethod\n",
    "    def get_rf_hyperparameters() -> dict[str, int | float | str]:\n",
    "        return {\n",
    "            \"n_estimators\": 10,\n",
    "            \"max_depth\": 10,\n",
    "            \"min_samples_split\": 3,\n",
    "            \"min_samples_leaf\": 2,\n",
    "            \"max_features\": 'sqrt',\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"random_state\": 42,\n",
    "            \"criterion\": \"entropy\",\n",
    "            \"min_impurity_decrease\": 0.01,\n",
    "            \"bootstrap\": True\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_training(cls):\n",
    "        sol = cls()\n",
    "        X, y = x_train, y_train\n",
    "        full_features = sol.compute_all_features(X)\n",
    "\n",
    "        # Initial training on all features\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        max_count = counts.max()\n",
    "        rng = np.random.default_rng(42)\n",
    "        full_features_bal = []\n",
    "        y_bal = []\n",
    "        for cls, count in zip(unique_classes, counts):\n",
    "            idxs = np.where(y == cls)[0]\n",
    "            if count < max_count:\n",
    "                extra = rng.choice(idxs, max_count - count, replace=True)\n",
    "                idxs = np.concatenate([idxs, extra])\n",
    "            full_features_bal.append(full_features[idxs])\n",
    "            y_bal.append(np.full(idxs.shape, cls))\n",
    "        full_features = np.vstack(full_features_bal)\n",
    "        y = np.concatenate(y_bal)\n",
    "        rf_full = RandomForestClassifier(**sol.get_rf_hyperparameters())\n",
    "        rf_full.fit(full_features, y)\n",
    "\n",
    "        # Select top 4 features (descending importance)\n",
    "        importances = rf_full.feature_importances_\n",
    "        top4 = np.argsort(importances)[::-1][:4]\n",
    "        sol.selected_indices = top4\n",
    "\n",
    "        # Retrain on top 4 features\n",
    "        sol.random_forest = RandomForestClassifier(**sol.get_rf_hyperparameters())\n",
    "        sol.random_forest.fit(full_features[:, top4], y)\n",
    "\n",
    "        # Display feature importances\n",
    "        print(\"Selected features:\", [sol.feature_names[i] for i in top4])\n",
    "        plt.figure()\n",
    "        plt.bar([sol.feature_names[i] for i in top4], importances[top4])\n",
    "        plt.title(\"Top 4 Feature Importances\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return sol\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_feature_distributions(meta_features: np.ndarray, labels: np.ndarray, class_names=None):\n",
    "    n_features = meta_features.shape[1]\n",
    "    n_classes = len(np.unique(labels))\n",
    "    class_names = class_names or [f'Class {i}' for i in range(n_classes)]\n",
    "\n",
    "    for i in range(n_features):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        for cls in range(n_classes):\n",
    "            cls_mask = labels == cls\n",
    "            plt.hist(meta_features[cls_mask, i], bins=30, alpha=0.6, label=class_names[cls], density=True)\n",
    "        plt.title(f\"Feature {i} Distribution\")\n",
    "        plt.xlabel(f\"Feature {i}\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ],
   "id": "385e0ea8f1eb2ead",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2e3614d",
   "metadata": {},
   "source": [
    "!pip install seaborn\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    f1_score, accuracy_score\n",
    ")\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    exemplary_solution = ExemplarySolution.create_with_training()\n",
    "    print(f\"Ocena: {score_solution(exemplary_solution)} pkt\")\n",
    "\n",
    "    # Compute meta-features and predictions on validation set\n",
    "    X_meta = exemplary_solution.compute_meta_features(x_valid)\n",
    "    y_pred = exemplary_solution.random_forest.predict(X_meta)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    print(\"\\n=== Classification Report ===\")\n",
    "    print(classification_report(y_valid, y_pred, target_names=[\n",
    "        \"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"\n",
    "    ]))\n",
    "\n",
    "    print(\"=== Confusion Matrix ===\")\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\n",
    "        \"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"\n",
    "    ])\n",
    "    disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Plot how each feature separates the classes\n",
    "plot_feature_distributions(X_meta, y_valid, class_names=[\n",
    "    \"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"\n",
    "])\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    # Train and predict\n",
    "    exemplary_solution = ExemplarySolution.create_with_training()\n",
    "    print(f\"\\nüéØ Ocena: {score_solution(exemplary_solution):.4f} pkt\")\n",
    "\n",
    "    X_meta = exemplary_solution.compute_meta_features(x_valid)\n",
    "    y_pred = exemplary_solution.random_forest.predict(X_meta)\n",
    "\n",
    "    class_labels = [\"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"]\n",
    "\n",
    "    # === üìä Classification Report ===\n",
    "    print(\"\\n=== üìä Classification Report ===\")\n",
    "    report = classification_report(y_valid, y_pred, target_names=class_labels)\n",
    "    print(report)\n",
    "\n",
    "    # === üîÄ Confusion Matrix (Absolute + Normalized) ===\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    ax.set_title(\"Confusion Matrix (Counts)\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Greens\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    ax.set_title(\"Confusion Matrix (Normalized)\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # === üß™ Per-class Accuracy & F1 ===\n",
    "    print(\"\\n=== üßÆ Per-Class Accuracy and F1 Score ===\")\n",
    "    for idx, label in enumerate(class_labels):\n",
    "        true = y_valid == idx\n",
    "        pred = y_pred == idx\n",
    "        acc = accuracy_score(true, pred)\n",
    "        f1 = f1_score(y_valid, y_pred, labels=[idx], average=\"macro\")\n",
    "        print(f\"{label:>12}: Accuracy = {acc:.2f} | F1 = {f1:.2f}\")\n",
    "\n",
    "    # Optional: Save confusion matrix to file\n",
    "    # fig.savefig(\"confusion_matrix_normalized.png\")\n",
    "\n",
    "    # === üìà Feature Distributions ===\n",
    "    plot_feature_distributions(X_meta, y_valid, class_names=class_labels)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d246838dce737412",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Twoje RozwiƒÖzanie\n",
    "\n",
    "W tej sekcji nale≈ºy umie≈õciƒá Twoje rozwiƒÖzanie. Wprowadzaj zmiany wy≈ÇƒÖcznie tutaj! XD"
   ]
  },
  {
   "cell_type": "code",
   "id": "182ac3bfb15bdb9c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "class YourSolution(ISolution):\n",
    "    def compute_meta_features(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Wyznacza 4 cechy fizjologicznie zwiƒÖzane z EKG:\n",
    "        - Szczytowe wychylenie (max)\n",
    "        - G≈Çƒôboko≈õƒá najni≈ºszego punktu (min)\n",
    "        - Rozstƒôp (peak-to-peak)\n",
    "        - Odchylenie standardowe (zmienno≈õƒá sygna≈Çu)\n",
    "        \"\"\"\n",
    "        max_vals = np.max(x, axis=1)\n",
    "        min_vals = np.min(x, axis=1)\n",
    "        ptp_vals = np.ptp(x, axis=1)\n",
    "        std_vals = np.std(x, axis=1)\n",
    "        \n",
    "        return np.column_stack((max_vals, min_vals, ptp_vals, std_vals))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_rf_hyperparameters() -> dict[str, int | float | str]:\n",
    "        \"\"\"\n",
    "        Zgodne z ograniczeniami: maks. 10 drzew i g≈Çƒôboko≈õƒá maks. 10.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"n_estimators\": 10,\n",
    "            \"max_depth\": 10,\n",
    "            \"min_samples_split\": 2,\n",
    "            \"min_samples_leaf\": 1,\n",
    "            \"max_features\": \"sqrt\",\n",
    "            \"bootstrap\": True\n",
    "        }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ce82420",
   "metadata": {},
   "source": [
    "# Ewaluacja\n",
    "\n",
    "Uruchomienie poni≈ºszej kom√≥rki pozwoli sprawdziƒá, ile punkt√≥w zdoby≈Çoby Twoje rozwiƒÖzanie na danych walidacyjnych. Przed wys≈Çaniem upewnij siƒô, ≈ºe ca≈Çy notebook wykonuje siƒô od poczƒÖtku do ko≈Ñca bez b≈Çƒôd√≥w i bez konieczno≈õci ingerencji u≈ºytkownika po wybraniu opcji \"Run All\".\n",
    "\n",
    "Podczas sprawdzania model zostanie zapisany jako `your_model.pkl` i oceniony na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "id": "686d3d51d2952d95",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOM√ìRKI PODCZAS WYSY≈ÅANIA ##########################\n",
    "if FINAL_EVALUATION_MODE:\n",
    "    your_solution = YourSolution.create_with_training()\n",
    "    print(f\"Ocena: {score_solution(your_solution)} pkt\")\n",
    "\n",
    "    OUTPUT_PATH = \"file_output\"\n",
    "    FUNCTION_FILENAME = \"your_solution\"\n",
    "    FUNCTION_OUTPUT_PATH = os.path.join(OUTPUT_PATH, FUNCTION_FILENAME)\n",
    "\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "    \n",
    "    with open(\"file_output/your_model.pkl\", \"wb\") as model_out:\n",
    "        cloudpickle.dump(your_solution, model_out)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b4ec356e54e32f5f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

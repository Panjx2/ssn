{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fee790bc89cb036",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Wykrywanie Zaburzeń Sygnału EKG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6fec6",
   "metadata": {},
   "source": [
    "![ECG_anomaly_detection_intro.png](https://live.staticflickr.com/65535/54253200317_77c251c48c_o.png)\n",
    "\n",
    "*Obraz wygenerowany przy użyciu modelu DALL-E.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc76c62a8c7e5e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Wstęp\n",
    "\n",
    "Rozwój sztucznej inteligencji otwiera nowe możliwości w diagnostyce medycznej, zwłaszcza w analizie złożonych danych, takich jak [sygnały elektrokardiograficzne (EKG)](https://www.healio.com/cardiology/learn-the-heart/ecg-review/ecg-interpretation-tutorial). EKG to jedno z najczęściej stosowanych narzędzi diagnostycznych w medycynie, umożliwiające ocenę i wykrywanie nieprawidłowości pracy serca.\n",
    "\n",
    "Tradycyjnie sygnał EKG powstaje z dwunastu odprowadzeń, jednak w tym zadaniu skupimy się na sygnale jednoodprowadzeniowym, czyli dysponujemy\n",
    "jedną zmienną reprezentującą napięcie elektryczne generowane przez serce w czasie. Dane te są rejestrowane w postaci krzywej zależnej od czasu, czyli można tutaj mówić o szeregu czasowym. Z sygnału EKG można wydzielić charakterystyczne fragmenty, czyli **załamki** (ang. waves) P, Q, R, S, T oraz **odstępy** pomiędzy dwoma zdarzeniami w EKG (ang. intervals), spośród których istotną rolę odgrywa odstęp R-R (czas między wystąpieniem dwóch kolejnych załamków R). Oprócz tego mówimy o **odcinkach** (ang. segments), czyli długości między dwoma określonymi załamkami w EKG, pomiędzy którymi powinna występować bazowa amplituda sygnału. Z kolei **zespół** (ang. complex) stanowi kilka zgrupowanych załamków. Głównie wyrózniamy tutaj zespół QRS. Schematyczny rysunek EKG wraz z podpisanymi fragmentami, jest prezentowany poniżej.\n",
    "\n",
    "![ECG.png](https://live.staticflickr.com/65535/54254099351_213d47784d_o.png)\n",
    "\n",
    "W EKG pochodzącym od zdrowej osoby można zauważyć sekwencję PQRST. Na początku wyróżniamy załamek P, która reprezentuje skurcz przedsionków i jest małym pionowym wychyleniem przed zespołem QRS. Następnie, zespół QRS wskazuje na skurcz komór i tworzony jest przez trzy wygięcia: załamek Q, załamek R oraz załamek S. Dalej można zauważyć odcinek ST, czyli płaski odcinek między zespołem QRS a załamkiem T, który odpowiada wczesnej fazie repolaryzacji komór. Finalnie, załamek T, który jest zaokrąglonym, pionowym wychyleniem, dotyczy repolaryzacji komór i ich powrotu do wyjściowego stanu.  Sekwencja PQRST przypomina sinusoidę, której maksimum jest osiągane dla załamka R. W przypadku zaburzeń pracy serca, EKG może wykazywać różne anomalie, takie jak dodatkowe minima lub maksima, czy też znacznie zwiększone odchylenie standardowe podczas całego pomiaru. Charakterystyka tych anomalii zależy od rodzaju i przyczyny zaburzenia.\n",
    "\n",
    "W poniższym zadaniu musisz zmierzyć się z próbkami zawierającymi pojedyncze sekwencje PQRST oraz ich okolice. Większość próbek będzie odpowiadała danym bez anomalii, których przykład zaprezentowany jest na poniższym obrazku:\n",
    "\n",
    "![normal_example.png](https://live.staticflickr.com/65535/54253200322_f0173af129_o.png)\n",
    "\n",
    "Występować będą też pomiary odpowiadające czterem rodzajom zaburzeń:  **AFib**, czyli migotaniu przedsionków, **PAC**, czyli przedwczesnemu pobudzeniu przedsionkowemu, **PVC**, czyli przedwczesnym skurczom komorowym oraz **STEMI**, czyli zawałowi mięśnia sercowego z uniesieniem odcinka ST.\n",
    "\n",
    "**UWAGA**: Poniższe dane są danymi syntetycznymi i są tylko pewnym przybliżeniem rzeczywistych danych EKG!\n",
    "\n",
    "EKG jest typowym przykładem szeregu czasowego, który można analizować za pomocą dedykowanych metod uczenia maszynowego, w tym sieci neuronowych, np. sieci rekurencyjnych. Jednak nie zawsze wykorzystanie sieci neuronowych jest konieczne, a nawet wskazane. W przypadku niektórych problemów satysfakcjonujące wyniki można uzyskać za pomocą prostszych metod, gdzie kluczowe jest odpowiednie przygotowanie danych. Ich umiejętna analiza pozwala na selekcję kliku metacech - cech zwięźle opisujących próbki ze zbioru danych, np. średniej, minimum, maksimum, odchylenia standardowego, itp. Mogą być one wykorzystane do klasyfikacji zamiast oryginalnych cech. Dzięki temu korzystamy z niskowymiarowych danych wejściowych, przykładowo redukujemy 150-wymiarowy wektor zawierający informacje z oryginalnych kroków czasowych do wektora 4-wymiarowego zawierającego specjalnie przygotowane cechy.\n",
    "\n",
    "Przykładem zastosowania niewielkiej liczby metacech są modele uczenia maszynowego, które mają działać na urządzeniach wbudowanych lub małych urządzeniach mobilnych, gdzie kluczowe są takie ograniczenia, jak wymóg niskiego poboru energii, mała ilość dostępnej pamięci operacyjnej czy ograniczona moc obliczeniowa. W takich przypadkach wymagane jest zastosowanie prostszych modeli, które są w stanie zapewnić odpowiednią dokładność klasyfikacji przy jednoczesnym zachowaniu wymaganych ograniczeń.\n",
    "\n",
    "## Zadanie\n",
    "\n",
    "Przygotuj rozwiązanie (wraz z wytrenowaniem modelu lasu losowego), które spełni wymagania naszego urządzenia wbudowanego. Przeanalizuj dane i przygotuj zestaw **4 metacech**, które dadzą najlepszą zrównoważoną dokładność (ang. *balanced accuracy*) dla problemu klasyfikacji sygnałów EKG. Zbiór danych składa się ze zbioru treningowego oraz walidacyjnego (wraz z etykietami), na którym możesz weryfikować swoje podejście. Twoje rozwiązanie będzie sprawdzane na osobnym (tajnym) zbiorze testowym, w którym liczba obserwacji będzie się różnić od liczby obserwacji w zbiorach treningowym i walidacyjnym. Każda próbka jest opisana 150 wartościami odpowiadającymi kolejnym krokom czasowym oraz jest przypisana do jednej z pięciu następujących klas:\n",
    "\n",
    "| ID klasy  | Nazwa klasy   | Opis  | Próbki w zbiorze treningowym | Próbki w zbiorze walidacyjnym |\n",
    "| ------    | ------        | ----  | ---------------------------- | ----------------------------- |\n",
    "| 0         | normal        | brak anomalii | 1400 | 819 |\n",
    "| 1         | afib          | Atrial Fibrillation (migotanie przedsionków)| 150 | 142 |\n",
    "| 2         | pac           | Premature Atrial Contractions (przedwczesne pobudzenie przedsionkowe)| 150 | 191 |\n",
    "| 3         | pvc           | Premature Ventricular Complex (przedwczesne skurcze komorowe)| 150 | 197 |\n",
    "| 4         | st_elevation  | ST-elevation myocardial infarction (zawał mięśnia sercowego z uniesieniem odcinka ST) | 150 | 151 |\n",
    "\n",
    "Nowe cechy powinny zawierać kluczowe informacje diagnostyczne róznicujące powyższe klasy, które pozwolą na skuteczną klasyfikację wymienionych anomalii.\n",
    "\n",
    "Klasyfikatorem dla tego zadania jest [las losowy](https://pl.wikipedia.org/wiki/Las_losowy) z liczbą drzew decyzyjnych nie większą niż 10 oraz maksymalną głębokością 10. **Rozwiązania niespełniające tych warunków będą dyskwalifikowane!** W przypadku innych parametrów lasu nie ma ograniczeń. Dozwolony jest także preprocessing, czyli wstępne przetwarzanie danych wejściowych (np. zastosowanie normalizacji danych).\n",
    "\n",
    "### Kryterium Oceny\n",
    "\n",
    "Twoje rozwiązanie oceniane będzie na tajnym zbiorze testowym na podstawie [zrównoważonej dokładności klasyfikacji (balanced accuracy)](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.balanced_accuracy_score.html):\n",
    "\n",
    "$$\\text{score}(balanced\\_accuracy) = \n",
    "\\begin{cases} \n",
    "    0 &\\quad \\text{jeżeli }  balanced\\_accuracy \\leq 75 \\% \\\\\n",
    "    100 &\\quad \\text{jeżeli }  balanced\\_accuracy \\geq 98 \\% \\\\\n",
    "    \\dfrac{balanced\\_accuracy - 75 \\%}{98 \\% - 75 \\%} &\\quad \\text{w pozostałych przypadkach}\n",
    "\\end{cases}$$\n",
    "\n",
    "Oznacza to, że wszystkie rozwiązania, które na zbiorze testowym uzyskają do $75\\%$ zrównoważonej dokładności klasyfikacji, otrzymają $0$ punktów, zaś co najmniej $98\\%$ zrównoważonej dokładności klasyfikacji, uzyskają maksymalną liczbę punktów za zadanie. Wszystkie zaś wartości z przedziału $75-98\\%$ zostaną zamienione na liczbę punktów (między $0$ a $100$) zgodnie z powyższym wzorem.\n",
    "\n",
    "*Wskazówka*: Twoim wyznacznikiem jakości proponowanego rozwiązania powinien być wynik na zbiorze walidacyjnym.\n",
    "\n",
    "W zagadnieniach dot. wykrywania chorób dość często mamy do czynienia z niezrównoważonym (niezbalansowanym) zbiorem danych. Chodzi o to, że zazwyczaj wśród danych dominują przykłady *normalne*, odpowiadające osobom zdrowym, a próbki reprezentujące osoby chore zwykle należą do mniejszości. Wyobraźmy sobie sytuację, w której na 100 próbek jedynie 10 dotyczy osób chorych, a pozostałe 90 zdrowych. Wówczas model, który każdej próbce przyporządkowywałby klasę *zdrowy*, osiągnąłby 90% dokładności klasyfikacji, lecz jedynie 50% zrównoważonej dokładności klasyfikacji! Oczywiście taki model byłby bezużyteczny. W takich przypadkach potrzebujemy miary, która lepiej odpowiada potrzebom wynikającym z postawionego problemu i informuje o skuteczności modelu w sposób użyteczny z punktu widzenia jego późniejszego użytkownika.\n",
    "\n",
    "W tym zadaniu musisz się więc skupić się na tym, by każda z klas była przyporządkowywana prawidłowo.\n",
    "\n",
    "## Ograniczenia\n",
    "- Twoje rozwiazanie będzie testowane na Platformie Konkursowej bez dostępu do internetu oraz w środowisku bez GPU.\n",
    "- Ewaluacja Twojego finalnego rozwiązania na Platformie Konkursowej nie może trwać dłużej niż 1 minutę bez GPU.\n",
    "- Podczas przygotowania danych należy pamiętać o tym, że:\n",
    "    - zakazane jest korzystanie z innych niż lasy losowe metod uczenia maszynowego, zarówno nadzorowanego jak i nienadzorowanego (np. autokodery, wielowarstwowe perceptrony i inne sieci neuronowe, maszyny wektorów nośnych (SVM), i inne), dozwolone są jednak metody redukcji wymiarowości, w stylu analizy składowych głównych (PCA);\n",
    "    - przy konstrukcji metacech można korzystać wyłącznie z funkcji dostępnych standardowo w Pythonie (`v3.11`), a także Numpy (`v2.0.2`) oraz Scipy (`v1.14.1`);\n",
    "    - można wyznaczyć maksymalnie 4 metacechy,\n",
    "- Do klasyfikacji można wykorzystać wyłącznie [las losowy (RandomForestClassifier) z biblioteki scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) (`v1.5.2`):\n",
    "    - złożony z maksymalnie 10 drzew decyzyjnych (`n_estimators` $\\leq 10$);\n",
    "    - każde drzewo ma mieć maksymalną głębokość równą 10 (`max_depth` $\\leq 10$);\n",
    "    - pozostałe hiperparametry można modyfikować bez ograniczeń;\n",
    "\n",
    "## Pliki Zgłoszeniowe\n",
    "Ten notebook uzupełniony o Twoje rozwiązanie (patrz klasa `YourSolution`), w którym przygotujesz zestaw 4 metacech opisujących zbiór danych oraz zestaw hiperparametrów lasu losowego.\n",
    "\n",
    "## Ewaluacja\n",
    "Pamiętaj, że podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`.\n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy 0 a 100 punktów. Liczba punktów, którą zdobędziesz, będzie wyliczona na (tajnym) zbiorze testowym na Platformie Konkursowej na podstawie wyżej wspomnianego wzoru, zaokrąglona do liczby całkowitej. Jeśli Twoje rozwiązanie nie będzie spełniało powyższych kryteriów lub nie będzie wykonywać się prawidłowo, otrzymasz za zadanie 0 punktów.\n",
    "\n",
    "---\n",
    "\n",
    "## Informacje Uzupełniające\n",
    "\n",
    "### Zrównoważona Dokładność Klasyfikacji\n",
    "\n",
    "Niech $C$ będzie liczbą klas, a $N_j$ odpowiada ilości próbek należących do $j$-tej klasy, gdzie $j \\in \\lbrace 1, ..., C \\rbrace$. Ponadto, niech $\\hat{y}_{i,j}$ będzie przewidywaną przez model klasą dla $j$-tej próbki należącej w rzeczywistości do $i$-tej klasy. Wówczas zrównoważoną (zbalansowaną) dokładność klasyfikacji możemy wyliczyć następująco:\n",
    "\n",
    "$$\n",
    "balanced\\_accuracy = \\dfrac{1}{C} \\sum\\limits_{i=1}^{C} \\sum\\limits_{j=1}^{|N_c|} \\dfrac{1}{|N_c|} \\cdot \\mathbf{1} \\left( \\hat{y}_{i, j} = i \\right),\n",
    "$$\n",
    "\n",
    "gdzie $\\mathbf{1} \\left( \\hat{y}_{i, j} = i \\right)$ jest funkcją indykatorową, która przyjmuje wartość 1, jeśli $\\hat{y}_{i, j} = i$, czyli w sytuacji, w której klasa przewidywana dla $j$-tej próbki jest taka sama jak rzeczywista klasa tej próbki oraz 0 w przeciwnym przypadku. Suma zewnętrzna przebiega po kolejnych klasach, a wewnętrzna po kolejnych próbkach należących do danej klasy.\n",
    "\n",
    "**Przykład**: Niech\n",
    "$$\\mathbf{y} = [0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3]$$\n",
    "\n",
    "będzie wektorem reprezentującym rzeczywiste klasy dla kolejnych próbek, a\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = [0, 0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3]$$\n",
    "\n",
    "wektorem reprezentującym predykcje modelu dla tychże próbek. Mamy więc do czynienia z czterema klasami, gdzie model miał problem z klasą o numerze $1$. Wszystkie pozostałe przykłady zostały przypisane bezbłędnie. Łącznie 10 na 12 próbek zostało sklasyfikowanych prawidłowo, co oznacza, że gdybyśmy mieli mierzyć \"zwykłą\" dokładność klasyfikacji, otrzymalibyśmy ok. $83.3\\%$ . Jednak gdy przyjrzymy się zbalansowanej dokładności klasyfikacji, otrzymamy wynik $75\\%$.\n",
    "\n",
    "Załóżmy teraz, że\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = [0, 0, 0, 1, 0, 2, 2, 2, 2, 3, 3, 3]$$\n",
    "\n",
    "czyli model klasyfikuje poprawnie $50\\%$ próbek z klasy 1 oraz $100\\%$ próbek z pozostałych klas. \"Zwykła\" dokładność klasyfikacji wynosi tutaj niespełna $92\\%$, podczas gdy zbalansowana dokładność klasyfikacji wynosi $87.5\\%$.\n",
    "\n",
    "### Anomalie Występujące w Rozważanym Zbiorze Danych\n",
    "\n",
    "#### AFib\n",
    "**Atrial Fibrillation (AFib)**, czyli migotanie przedsionków, występuje wtedy, gdy potencjały czynnościowe wyzwalają się bardzo szybko i chaotycznie, w związku z tym rytm serca jest nieregularny. W tym zaburzeniu załamki P mogą nie być widoczne w EKG, a zespół QRS staje się nieregularny.\n",
    "\n",
    "![AFIB_example.png](https://live.staticflickr.com/65535/54253219357_78c51f06ff_o.png)\n",
    "\n",
    "#### PAC\n",
    "**Premature Atrial Contractions (PACs)**, czyli przedwczesne pobudzenie przedsionkowe, związane jest z nieprawidłowym załamkiem P, po którym następuje prawidłowy zespół QRS. *Uwaga!* W próbkach z zadania występują przykłady, w których w ramach jednej próbki ze zbioru danych widoczny jest jedynie przedwczesny załamek P.\n",
    "\n",
    "![PAC_example.png](https://live.staticflickr.com/65535/54254324138_df77fdd62a_o.png)\n",
    "\n",
    "#### PVC\n",
    "**Premature Ventricular Contractions (PVCs)**, czyli przedwczesne skurcze komorowe, są dodatkowymi uderzeniami serca, które rozpoczynają się w jednej z dwóch komór serca i zakłócają jego regularny rytm. Są jednym z powszechnych rodzajów arytmii. Skurcze te zachodzą wcześniej niż byłoby to oczekiwane na podstawie poprzednich odstępów R-R.\n",
    "\n",
    "![PVC_example.png](https://live.staticflickr.com/65535/54254518540_12ba23c53f_o.png)\n",
    "\n",
    "#### STEMI\n",
    "**ST-elevation myocardial infarction (STEMI)**, czyli zawał mięśnia sercowego z uniesieniem odcinka ST, powoduje zablokowanie przepływu krwi do mięśnia sercowego i jego obumarcie. Segment ST występuje tuż po zespole QRS. W normalnej sytuacji nie ma tam żadnej aktywności elektrycznej, przez co jest on płaski. Jeśli zaś występuje uniesienie odcinka ST, oznacza to blokadę jednej z głównych tętnic doprowadzających krew do serca.\n",
    "\n",
    "![STEMI_example.png](https://live.staticflickr.com/65535/54254099356_422c23144e_o.png)\n",
    "\n",
    "---\n",
    "\n",
    "**Źródła opisu medycznego dot. EKG**: [1](https://www.healio.com/cardiology/learn-the-heart/ecg-review/ecg-topic-reviews-and-criteria/premature-ventricular-contractions-review), [2](https://www.mayoclinic.org/diseases-conditions/premature-ventricular-contractions/symptoms-causes/syc-20376757), [3](https://litfl.com/premature-atrial-complex-pac/), [4](https://www.healio.com/cardiology/learn-the-heart/ecg-review/ecg-topic-reviews-and-criteria/premature-atrial-contractions-review), [5](https://www.mayoclinic.org/diseases-conditions/atrial-fibrillation/symptoms-causes/syc-20350624), [6](https://www.healio.com/cardiology/learn-the-heart/ecg-review/ecg-topic-reviews-and-criteria/atrial-fibrillation-review), [7](https://my.clevelandclinic.org/health/diseases/22068-stemi-heart-attack), obraz załamków PQRST na podstawie [8](https://www.sciencedirect.com/science/article/pii/S0213911121002466)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221e864-03ae-4230-8a37-bfec058ff3ac",
   "metadata": {},
   "source": [
    "# Kod Startowy\n",
    "\n",
    "W tej sekcji inicjalizujemy środowisko poprzez zaimportowanie potrzebnych bibliotek i funkcji. Przygotowany kod ułatwi Tobie efektywne operowanie na danych i budowanie właściwego rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "id": "a4053c4df3efc18b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI PODCZAS WYSYŁANIA ##########################\n",
    "\n",
    "# W czasie sprawdzania Twojego rozwiązania, wartość flagi FINAL_EVALUATION_MODE zostanie zmieniona na True\n",
    "FINAL_EVALUATION_MODE = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8511460ed63f204e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI PODCZAS WYSYŁANIA ##########################\n",
    "import cloudpickle\n",
    "\n",
    "import os\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82c4f388e11fc3ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI PODCZAS WYSYŁANIA ##########################\n",
    "\n",
    "# Ustawienie ziarna generatora liczb pseudolosowych w celu zapewnienia deterministyczności wyników.\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6ae949692319123",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Ładowanie Danych\n",
    "Za pomocą poniższego kodu wczytujemy dane."
   ]
  },
  {
   "cell_type": "code",
   "id": "aceaa260cc57432c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI PODCZAS WYSYŁANIA ##########################\n",
    "\n",
    "train_val_filename = \"train_validation_sets.npz\"\n",
    "if not os.path.exists(train_val_filename):\n",
    "    import gdown\n",
    "    url = \"https://drive.google.com/file/d/1pCqgbsKBQP1UnH2kMmBKRS1AuvmSl9jx/view?usp=sharing\"\n",
    "    gdown.download(url, train_val_filename, quiet=True, fuzzy=True)\n",
    "\n",
    "train_valid_bundle = np.load(\"train_validation_sets.npz\", allow_pickle=True)\n",
    "x_train = train_valid_bundle[\"X_train\"]\n",
    "y_train = train_valid_bundle[\"y_train\"]\n",
    "y_train_str = train_valid_bundle[\"anomaly_train\"]\n",
    "\n",
    "x_valid = train_valid_bundle[\"X_validation\"]\n",
    "y_valid = train_valid_bundle[\"y_validation\"]\n",
    "y_valid_str = train_valid_bundle[\"anomaly_validation\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39ff3878366476e8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Publiczny Interfejs Rozwiązania\n",
    "\n",
    "Tylko tego wymagamy od Twojej klasy. W Twoim rozwiązaniu możesz modyfikować swoją klasę do woli dodając atrybuty oraz nowe metody obliczające metacechy - cokolwiek co będzie Ci potrzebne do rozwiązania zadania."
   ]
  },
  {
   "cell_type": "code",
   "id": "55f895fcd12becb3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI PODCZAS WYSYŁANIA ##########################\n",
    "\n",
    "class ISolution(ABC):\n",
    "    random_forest: RandomForestClassifier | None = None\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_training(cls) -> \"ISolution\":\n",
    "        \"\"\"Metoda służąca do stworzenia rozwiązania z wytrenowanym lasem losowym.\"\"\"\n",
    "        solution = cls()\n",
    "\n",
    "        hyperparameters = cls.get_rf_hyperparameters()\n",
    "        hyperparameters = cls.validate_hyperparameters(hyperparameters)\n",
    "        solution.random_forest = RandomForestClassifier(**hyperparameters)\n",
    "\n",
    "        meta_features = solution.compute_meta_features(x_train)\n",
    "        solution.random_forest.fit(meta_features, y_train)\n",
    "        return solution\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_hyperparameters(hyperparameters: dict[str, int | float | str]) -> dict[str, int | float | str]:\n",
    "        \"\"\"\n",
    "        Funkcja ta sprawdza, czy hiperparametry lasu losowego są zgodne z wymaganiami zadania. Jeśli nie, to poprawia je na\n",
    "        domyślne wartości.\n",
    "        \"\"\"\n",
    "        hyperparameters[\"n_estimators\"] = min(hyperparameters.get(\"n_estimators\", 10), 10)\n",
    "        hyperparameters[\"max_depth\"] = min(hyperparameters.get(\"max_depth\", 10), 10)\n",
    "        hyperparameters[\"random_state\"] = 42\n",
    "        return hyperparameters\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_meta_features(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Funkcja ta powinna dla każdego przykładu ze zbioru $x$ opisanego 150 cechami zwrócić wektor 4 cech, który będzie\n",
    "        reprezentował ten przykład. Funkcja ta powinna przekształcać wejściową tablicę o rozmiarze (n, 150) na tablicę o\n",
    "        rozmiarze (n, 4).\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def get_rf_hyperparameters() -> dict[str, int | float | str]:\n",
    "        \"\"\"\n",
    "        Funkcja ta powinna zwracać słownik z hiperparametrami lasu losowego. Pamiętaj o ograniczeniach na liczbę drzew i ich\n",
    "        głębokość!\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "76fa659473771fa",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Kod z Kryterium Oceniającym\n",
    "Kod, zbliżony do poniższego, będzie używany do oceny rozwiązania na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "id": "621097e2336ffec3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI PODCZAS WYSYŁANIA ##########################\n",
    "\n",
    "def balanced_accuracy_to_score(balanced_accuracy: float) -> float:\n",
    "    return min(max((balanced_accuracy - 75.) * (100. / (98. - 75.)), 0.), 100.)\n",
    "\n",
    "\n",
    "def score_solution(solution: ISolution) -> float:\n",
    "    x, y = x_valid, y_valid\n",
    "    meta_features = solution.compute_meta_features(x)\n",
    "    y_hat = solution.random_forest.predict(meta_features)\n",
    "    balanced_accuracy = 100. * balanced_accuracy_score(y, y_hat)\n",
    "\n",
    "    assert meta_features.shape[-1] <= 4\n",
    "    assert solution.random_forest.n_estimators <= 10\n",
    "    assert solution.random_forest.max_depth <= 10\n",
    "\n",
    "    if not FINAL_EVALUATION_MODE:\n",
    "        print(\"Ocena działania modelu: \\n\")\n",
    "        print(f\"Zbalansowana dokładność klasyfikacji: {balanced_accuracy: .4f}\")\n",
    "    return int(round(balanced_accuracy_to_score(balanced_accuracy)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# from scipy.signal import find_peaks, peak_widths\n",
    "# from scipy.stats import variation, entropy, skew\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# \n",
    "# # Temporary class for dev — skip assert limits\n",
    "# class DebugSolution:\n",
    "#     def compute_meta_features(self, x):\n",
    "#         n = x.shape[0]\n",
    "#         features = np.zeros((n, 10))  # We compute 10 features here\n",
    "#         \n",
    "#         for i in range(n):\n",
    "#             sig = x[i]\n",
    "#             norm_sig = (sig - sig.min()) / (sig.max() - sig.min() + 1e-8)\n",
    "#             peaks, _ = find_peaks(sig, height=0.4, distance=20)\n",
    "#             rr = np.diff(peaks)\n",
    "# \n",
    "#             # Original + new features\n",
    "#             features[i, 0] = variation(rr) if len(rr) > 1 else 0  # RR CV\n",
    "#             features[i, 1] = entropy(norm_sig + 1e-8)              # Entropy\n",
    "#             features[i, 2] = np.abs(skew(sig))                    # Skewness\n",
    "#             features[i, 3] = np.std(rr) if len(rr) > 1 else 0     # RR Std\n",
    "#             features[i, 4] = len(peaks)                           # Peak count\n",
    "#             features[i, 5] = np.max(sig) - np.min(sig)            # Peak-to-peak amp\n",
    "#             features[i, 6] = np.median(np.diff(sig))              # Median diff\n",
    "#             features[i, 7] = np.percentile(sig, 90) - np.percentile(sig, 10)  # Spread\n",
    "#             features[i, 8] = np.mean(np.abs(np.diff(sig)))        # Mean abs diff\n",
    "#             features[i, 9] = np.max(sig[:50]) - np.median(sig[:50])  # P-wave bump\n",
    "# \n",
    "#         return features\n",
    "# \n",
    "# # === Load your dev data ===\n",
    "# x_train, y_train = x_train, y_train\n",
    "# x_valid, y_valid = x_valid, y_valid\n",
    "# \n",
    "# # === Compute features ===\n",
    "# model = DebugSolution()\n",
    "# X_train_full = model.compute_meta_features(x_train)\n",
    "# X_valid_full = model.compute_meta_features(x_valid)\n",
    "# \n",
    "# # === Feature selection ===\n",
    "# selector = SelectKBest(f_classif, k=4)\n",
    "# X_train_sel = selector.fit_transform(X_train_full, y_train)\n",
    "# X_valid_sel = selector.transform(X_valid_full)\n",
    "# \n",
    "# top_features = selector.get_support(indices=True)\n",
    "# print(\"Selected top feature indices:\", top_features)\n",
    "# \n",
    "# # === Train classifier ===\n",
    "# clf = RandomForestClassifier(n_estimators=10, max_depth=10, random_state=42)\n",
    "# clf.fit(X_train_sel, y_train)\n",
    "# y_pred = clf.predict(X_valid_sel)\n",
    "# \n",
    "# # === Metrics ===\n",
    "# print(\"\\n=== Classification Report ===\")\n",
    "# print(classification_report(y_valid, y_pred, target_names=[\"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"]))\n",
    "# \n",
    "# # === Confusion Matrix ===\n",
    "# cm = confusion_matrix(y_valid, y_pred, normalize='true')\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# sns.heatmap(cm, annot=True, cmap=\"viridis\", xticklabels=[\"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"],\n",
    "#             yticklabels=[\"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"])\n",
    "# plt.title(\"Normalized Confusion Matrix\")\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"True\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# \n",
    "# # === Feature Importances ===\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.bar(range(4), clf.feature_importances_)\n",
    "# plt.xticks(range(4), [f'F{i}' for i in top_features])\n",
    "# plt.title(\"Top 4 Feature Importances\")\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ],
   "id": "3388c31e1d294f11",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a866b78776f10fe9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Przykładowe Rozwiązanie\n",
    "\n",
    "Poniżej przedstawiamy uproszczone rozwiązanie, które służy jako przykład demonstrujący podstawową funkcjonalność notatnika. Może ono posłużyć jako punkt wyjścia do opracowania Twojego rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "id": "f561bb976552807f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "from scipy.stats import variation, skew, entropy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ExemplarySolution(ISolution):\n",
    "    def __init__(self):\n",
    "        self.selected_indices = None\n",
    "        self.random_forest = None\n",
    "        self.feature_names = [\n",
    "            \"RR_var\", \"Entropy\", \"Pwave_ratio\", \"P_early_dist\",\n",
    "            \"QRS_width\", \"QRS_sharp\", \"ST_elev\", \"Skew_abs\", \"Peak_count\",\n",
    "            \"RMS\", \"Kurtosis\", \"ST_slope\"\n",
    "        ]\n",
    "\n",
    "    def compute_all_features(self, x: np.ndarray) -> np.ndarray:\n",
    "        n_samples = x.shape[0]\n",
    "        features = np.zeros((n_samples, 12))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            signal = x[i]\n",
    "            norm_signal = (signal - np.min(signal)) / (np.max(signal) - np.min(signal) + 1e-8)\n",
    "\n",
    "            # R peaks\n",
    "            peaks, _ = find_peaks(signal, height=0.4, distance=20, prominence=0.2)\n",
    "            rr_intervals = np.diff(peaks) if len(peaks) >= 2 else np.array([1])\n",
    "\n",
    "            # === AFib ===\n",
    "            features[i, 0] = np.log1p(variation(rr_intervals)) if len(rr_intervals) > 1 else 0\n",
    "            features[i, 1] = entropy(norm_signal + 1e-8)\n",
    "\n",
    "            # === PAC ===\n",
    "            p_region = signal[:80]\n",
    "            p_peaks, p_props = find_peaks(p_region, height=0.2, prominence=0.1, width=5)\n",
    "            if len(p_peaks) > 0:\n",
    "                widths = peak_widths(p_region, p_peaks, rel_height=0.5)[0]\n",
    "                features[i, 2] = np.mean(p_props['prominences']) / (np.mean(widths) + 1e-6)\n",
    "                features[i, 3] = np.min(np.diff(p_peaks)) if len(p_peaks) > 1 else 0\n",
    "            else:\n",
    "                features[i, 2:4] = 0\n",
    "\n",
    "            # === PVC ===\n",
    "            qrs_peaks, _ = find_peaks(signal, height=0.5, distance=25, prominence=0.3)\n",
    "            if len(qrs_peaks) > 0:\n",
    "                main_peak = qrs_peaks[np.argmax(signal[qrs_peaks])]\n",
    "                width = peak_widths(signal, [main_peak], rel_height=0.5)[0][0]\n",
    "                features[i, 4] = width\n",
    "                features[i, 5] = signal[main_peak] / (width + 1e-6)\n",
    "            else:\n",
    "                features[i, 4:6] = 0\n",
    "                main_peak = np.argmax(signal[50:100]) + 50  # fallback\n",
    "\n",
    "            # === ST Elevation ===\n",
    "            qrs_end = main_peak + 15\n",
    "            st_region = signal[qrs_end:min(len(signal), qrs_end + 20)]\n",
    "            baseline = np.percentile(signal[:30], 25)\n",
    "            features[i, 6] = np.percentile(st_region, 90) - baseline\n",
    "\n",
    "            # === General ===\n",
    "            features[i, 7] = np.abs(skew(signal))\n",
    "            features[i, 8] = len(peaks)\n",
    "            features[i, 9] = np.sqrt(np.mean(signal**2))\n",
    "            features[i, 10] = kurtosis(signal)\n",
    "            st_len = len(st_region)\n",
    "            features[i, 11] = (st_region[-1] - st_region[0]) / (st_len if st_len else 1)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def compute_meta_features(self, x: np.ndarray) -> np.ndarray:\n",
    "        full = self.compute_all_features(x)\n",
    "        return full[:, self.selected_indices] if self.selected_indices is not None else full\n",
    "\n",
    "    @staticmethod\n",
    "    def get_rf_hyperparameters() -> dict[str, int | float | str]:\n",
    "        return {\n",
    "            \"n_estimators\": 10,\n",
    "            \"max_depth\": 10,\n",
    "            \"min_samples_split\": 3,\n",
    "            \"min_samples_leaf\": 2,\n",
    "            \"max_features\": 'sqrt',\n",
    "            \"class_weight\": \"balanced\",\n",
    "            \"random_state\": 42,\n",
    "            \"criterion\": \"entropy\",\n",
    "            \"min_impurity_decrease\": 0.01,\n",
    "            \"bootstrap\": True\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def create_with_training(cls):\n",
    "        sol = cls()\n",
    "        X, y = x_train, y_train\n",
    "        full_features = sol.compute_all_features(X)\n",
    "\n",
    "        # Initial training on all features\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        max_count = counts.max()\n",
    "        rng = np.random.default_rng(42)\n",
    "        full_features_bal = []\n",
    "        y_bal = []\n",
    "        for cls, count in zip(unique_classes, counts):\n",
    "            idxs = np.where(y == cls)[0]\n",
    "            if count < max_count:\n",
    "                extra = rng.choice(idxs, max_count - count, replace=True)\n",
    "                idxs = np.concatenate([idxs, extra])\n",
    "            full_features_bal.append(full_features[idxs])\n",
    "            y_bal.append(np.full(idxs.shape, cls))\n",
    "        full_features = np.vstack(full_features_bal)\n",
    "        y = np.concatenate(y_bal)\n",
    "        rf_full = RandomForestClassifier(**sol.get_rf_hyperparameters())\n",
    "        rf_full.fit(full_features, y)\n",
    "\n",
    "        # Select top 4 features (descending importance)\n",
    "        importances = rf_full.feature_importances_\n",
    "        top4 = np.argsort(importances)[::-1][:4]\n",
    "        sol.selected_indices = top4\n",
    "\n",
    "        # Retrain on top 4 features\n",
    "        sol.random_forest = RandomForestClassifier(**sol.get_rf_hyperparameters())\n",
    "        sol.random_forest.fit(full_features[:, top4], y)\n",
    "\n",
    "        # Display feature importances\n",
    "        print(\"Selected features:\", [sol.feature_names[i] for i in top4])\n",
    "        plt.figure()\n",
    "        plt.bar([sol.feature_names[i] for i in top4], importances[top4])\n",
    "        plt.title(\"Top 4 Feature Importances\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return sol\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_feature_distributions(meta_features: np.ndarray, labels: np.ndarray, class_names=None):\n",
    "    n_features = meta_features.shape[1]\n",
    "    n_classes = len(np.unique(labels))\n",
    "    class_names = class_names or [f'Class {i}' for i in range(n_classes)]\n",
    "\n",
    "    for i in range(n_features):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        for cls in range(n_classes):\n",
    "            cls_mask = labels == cls\n",
    "            plt.hist(meta_features[cls_mask, i], bins=30, alpha=0.6, label=class_names[cls], density=True)\n",
    "        plt.title(f\"Feature {i} Distribution\")\n",
    "        plt.xlabel(f\"Feature {i}\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ],
   "id": "385e0ea8f1eb2ead",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2e3614d",
   "metadata": {},
   "source": [
    "!pip install seaborn\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    f1_score, accuracy_score\n",
    ")\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    exemplary_solution = ExemplarySolution.create_with_training()\n",
    "    print(f\"Ocena: {score_solution(exemplary_solution)} pkt\")\n",
    "\n",
    "    # Compute meta-features and predictions on validation set\n",
    "    X_meta = exemplary_solution.compute_meta_features(x_valid)\n",
    "    y_pred = exemplary_solution.random_forest.predict(X_meta)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    print(\"\\n=== Classification Report ===\")\n",
    "    print(classification_report(y_valid, y_pred, target_names=[\n",
    "        \"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"\n",
    "    ]))\n",
    "\n",
    "    print(\"=== Confusion Matrix ===\")\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\n",
    "        \"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"\n",
    "    ])\n",
    "    disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
    "    plt.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Plot how each feature separates the classes\n",
    "plot_feature_distributions(X_meta, y_valid, class_names=[\n",
    "    \"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"\n",
    "])\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    # Train and predict\n",
    "    exemplary_solution = ExemplarySolution.create_with_training()\n",
    "    print(f\"\\n🎯 Ocena: {score_solution(exemplary_solution):.4f} pkt\")\n",
    "\n",
    "    X_meta = exemplary_solution.compute_meta_features(x_valid)\n",
    "    y_pred = exemplary_solution.random_forest.predict(X_meta)\n",
    "\n",
    "    class_labels = [\"normal\", \"afib\", \"pac\", \"pvc\", \"st_elevation\"]\n",
    "\n",
    "    # === 📊 Classification Report ===\n",
    "    print(\"\\n=== 📊 Classification Report ===\")\n",
    "    report = classification_report(y_valid, y_pred, target_names=class_labels)\n",
    "    print(report)\n",
    "\n",
    "    # === 🔀 Confusion Matrix (Absolute + Normalized) ===\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    ax.set_title(\"Confusion Matrix (Counts)\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Greens\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    ax.set_title(\"Confusion Matrix (Normalized)\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # === 🧪 Per-class Accuracy & F1 ===\n",
    "    print(\"\\n=== 🧮 Per-Class Accuracy and F1 Score ===\")\n",
    "    for idx, label in enumerate(class_labels):\n",
    "        true = y_valid == idx\n",
    "        pred = y_pred == idx\n",
    "        acc = accuracy_score(true, pred)\n",
    "        f1 = f1_score(y_valid, y_pred, labels=[idx], average=\"macro\")\n",
    "        print(f\"{label:>12}: Accuracy = {acc:.2f} | F1 = {f1:.2f}\")\n",
    "\n",
    "    # Optional: Save confusion matrix to file\n",
    "    # fig.savefig(\"confusion_matrix_normalized.png\")\n",
    "\n",
    "    # === 📈 Feature Distributions ===\n",
    "    plot_feature_distributions(X_meta, y_valid, class_names=class_labels)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d246838dce737412",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Twoje Rozwiązanie\n",
    "\n",
    "W tej sekcji należy umieścić Twoje rozwiązanie. Wprowadzaj zmiany wyłącznie tutaj! XD"
   ]
  },
  {
   "cell_type": "code",
   "id": "182ac3bfb15bdb9c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "class YourSolution(ISolution):\n",
    "    def compute_meta_features(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Wyznacza 4 cechy fizjologicznie związane z EKG:\n",
    "        - Szczytowe wychylenie (max)\n",
    "        - Głębokość najniższego punktu (min)\n",
    "        - Rozstęp (peak-to-peak)\n",
    "        - Odchylenie standardowe (zmienność sygnału)\n",
    "        \"\"\"\n",
    "        max_vals = np.max(x, axis=1)\n",
    "        min_vals = np.min(x, axis=1)\n",
    "        ptp_vals = np.ptp(x, axis=1)\n",
    "        std_vals = np.std(x, axis=1)\n",
    "        \n",
    "        return np.column_stack((max_vals, min_vals, ptp_vals, std_vals))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_rf_hyperparameters() -> dict[str, int | float | str]:\n",
    "        \"\"\"\n",
    "        Zgodne z ograniczeniami: maks. 10 drzew i głębokość maks. 10.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"n_estimators\": 10,\n",
    "            \"max_depth\": 10,\n",
    "            \"min_samples_split\": 2,\n",
    "            \"min_samples_leaf\": 1,\n",
    "            \"max_features\": \"sqrt\",\n",
    "            \"bootstrap\": True\n",
    "        }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2ce82420",
   "metadata": {},
   "source": [
    "# Ewaluacja\n",
    "\n",
    "Uruchomienie poniższej komórki pozwoli sprawdzić, ile punktów zdobyłoby Twoje rozwiązanie na danych walidacyjnych. Przed wysłaniem upewnij się, że cały notebook wykonuje się od początku do końca bez błędów i bez konieczności ingerencji użytkownika po wybraniu opcji \"Run All\".\n",
    "\n",
    "Podczas sprawdzania model zostanie zapisany jako `your_model.pkl` i oceniony na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "id": "686d3d51d2952d95",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI PODCZAS WYSYŁANIA ##########################\n",
    "if FINAL_EVALUATION_MODE:\n",
    "    your_solution = YourSolution.create_with_training()\n",
    "    print(f\"Ocena: {score_solution(your_solution)} pkt\")\n",
    "\n",
    "    OUTPUT_PATH = \"file_output\"\n",
    "    FUNCTION_FILENAME = \"your_solution\"\n",
    "    FUNCTION_OUTPUT_PATH = os.path.join(OUTPUT_PATH, FUNCTION_FILENAME)\n",
    "\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "    \n",
    "    with open(\"file_output/your_model.pkl\", \"wb\") as model_out:\n",
    "        cloudpickle.dump(your_solution, model_out)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b4ec356e54e32f5f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
